{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd34fbab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8499668b07488ba63dbdad8b07cab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = \"ministral/Ministral-3b-instruct\"\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "lora_model = PeftModel.from_pretrained(base, \"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "732a9290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-13): 14 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): MistralRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lora_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b8035eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\").shuffle(seed=42).select(range(2000))\n",
    "\n",
    "def make_prompt(x):\n",
    "    prompt = \"<|im_start|>user\\n\"\n",
    "    prompt += x[\"instruction\"]\n",
    "\n",
    "    if x[\"input\"].strip():\n",
    "        prompt += \"\\n\" + x[\"input\"]\n",
    "    \n",
    "    prompt += \"\\n<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    return prompt\n",
    "\n",
    "def tokenize_func(x):\n",
    "    text_tokens = tokenizer(\n",
    "        x[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    prompt = make_prompt(x)\n",
    "    prompt_tokens = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"text_input_ids\": text_tokens[\"input_ids\"],\n",
    "        \"text_attention_mask\": text_tokens[\"attention_mask\"],\n",
    "        \"prompt_input_ids\": prompt_tokens[\"input_ids\"],\n",
    "        \"prompt_attention_mask\": prompt_tokens[\"attention_mask\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dd0cf41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e349c6d50fb41c9a128d4454e5d39a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.025, seed=42)\n",
    "train = dataset['train']\n",
    "val = dataset['test']\n",
    "\n",
    "val = val.map(tokenize_func, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87189538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "Perplexity: 12523.647017364376\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import math \n",
    "\n",
    "lora_model.eval()\n",
    "total_loss = 0.0\n",
    "count = 0\n",
    "val_loader = DataLoader(val, batch_size=1)\n",
    "\n",
    "for batch in val_loader:\n",
    "    print(count)\n",
    "    input_ids = torch.tensor(batch['text_input_ids'], device=device)\n",
    "    attention_mask = torch.tensor(batch['text_attention_mask'], device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = lora_model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "    \n",
    "    total_loss += outputs.loss.item()\n",
    "    count += 1\n",
    "\n",
    "ppl = math.exp(total_loss / count)\n",
    "print(f\"Perplexity: {ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff86477c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: <|im_start|>user\n",
      "Describe a process to deploy a web app using an Amazon EC2 instance.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "Model output: <|im_start|>user\n",
      "Describe a process to deploy a web app using an Amazon EC2 instance.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "According to the instructions provided in the instructions provided, you will need to set up the S3 cluster. Next, the next step is to create an EC2 instance. Include your address using Amazon's pre-existing service account key and configure your environment variables. Finally, you can use the App Access token to access the application and make necessary changes. If you haven't done so, allow the applications to allow their users to install an App Access token, which you can access via your application source-source connection. Once you have allowed, proceed to launch the application, to enable any options, or let the app have your access token. Once the app is launched, you can access it using one of the other tools available. In case of error or other events, you can now replace the default settings. If any errors occurred during installation, install them more easily by adding them by adding the API key and app ownership to your application source-source connection. Once you have made any changes and the app is set up, use its service account key to create and access the API key. Once the App Access token is installed, you can use it to access the app on Google Store. You can now search for apps to install and download apps.\n",
      "\n",
      "Expected output: The process for deploying a web app on an Amazon EC2 instance involves several steps. First, you will need to create an Amazon account and set up an EC2 instance. Once the instance is created you will need to configure the instance with the necessary software and settings to run your web application. This includes installing any necessary packages, setting up the web server, and configuring the database. After the instance is ready, you can deploy your code by connecting to the EC2 instance via SSH and transferring the files. You will then need to configure the application and securely open the ports to external access. Finally, you will need to configure the DNS settings and then your web application should be ready to use.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in val_loader:\n",
    "    input_ids = torch.tensor(batch['prompt_input_ids'], device=device)\n",
    "    attention_mask = torch.tensor(batch['prompt_attention_mask'], device=device)\n",
    "    outputs = lora_model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"PROMPT: {batch['prompt'][0]}\", end = \"\\n\\n\")\n",
    "    print(f\"Model output: {decoded_output}\", end = \"\\n\\n\")\n",
    "    print(f\"Expected output: {batch['output'][0]}\", end = \"\\n\\n\")\n",
    "\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c0b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
