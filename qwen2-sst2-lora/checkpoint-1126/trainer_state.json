{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1126,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.014222222222222223,
      "grad_norm": 29.15114974975586,
      "learning_rate": 0.00019875666074600356,
      "loss": 0.9861,
      "step": 8
    },
    {
      "epoch": 0.028444444444444446,
      "grad_norm": 87.6139907836914,
      "learning_rate": 0.00019733570159857907,
      "loss": 1.1641,
      "step": 16
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 35.58747100830078,
      "learning_rate": 0.00019591474245115453,
      "loss": 0.9783,
      "step": 24
    },
    {
      "epoch": 0.05688888888888889,
      "grad_norm": 46.301116943359375,
      "learning_rate": 0.00019449378330373002,
      "loss": 0.7738,
      "step": 32
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 19.03059959411621,
      "learning_rate": 0.0001930728241563055,
      "loss": 0.7412,
      "step": 40
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 47.90141296386719,
      "learning_rate": 0.000191651865008881,
      "loss": 0.6205,
      "step": 48
    },
    {
      "epoch": 0.09955555555555555,
      "grad_norm": 40.152259826660156,
      "learning_rate": 0.00019023090586145648,
      "loss": 0.7446,
      "step": 56
    },
    {
      "epoch": 0.11377777777777778,
      "grad_norm": 40.931514739990234,
      "learning_rate": 0.00018880994671403197,
      "loss": 0.6273,
      "step": 64
    },
    {
      "epoch": 0.128,
      "grad_norm": 69.67351531982422,
      "learning_rate": 0.00018738898756660749,
      "loss": 0.6423,
      "step": 72
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 37.76213073730469,
      "learning_rate": 0.00018596802841918295,
      "loss": 0.4578,
      "step": 80
    },
    {
      "epoch": 0.15644444444444444,
      "grad_norm": 152.06008911132812,
      "learning_rate": 0.00018454706927175843,
      "loss": 0.8715,
      "step": 88
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 54.05868148803711,
      "learning_rate": 0.00018312611012433395,
      "loss": 0.5117,
      "step": 96
    },
    {
      "epoch": 0.18488888888888888,
      "grad_norm": 35.963069915771484,
      "learning_rate": 0.00018170515097690944,
      "loss": 0.4105,
      "step": 104
    },
    {
      "epoch": 0.1991111111111111,
      "grad_norm": 44.2626953125,
      "learning_rate": 0.0001802841918294849,
      "loss": 0.6701,
      "step": 112
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 25.549100875854492,
      "learning_rate": 0.0001788632326820604,
      "loss": 0.6828,
      "step": 120
    },
    {
      "epoch": 0.22755555555555557,
      "grad_norm": 21.78646469116211,
      "learning_rate": 0.0001774422735346359,
      "loss": 0.4987,
      "step": 128
    },
    {
      "epoch": 0.24177777777777779,
      "grad_norm": 20.876842498779297,
      "learning_rate": 0.00017602131438721136,
      "loss": 0.4125,
      "step": 136
    },
    {
      "epoch": 0.256,
      "grad_norm": 10.814919471740723,
      "learning_rate": 0.00017460035523978685,
      "loss": 0.4586,
      "step": 144
    },
    {
      "epoch": 0.2702222222222222,
      "grad_norm": 0.8945131301879883,
      "learning_rate": 0.00017317939609236236,
      "loss": 0.1277,
      "step": 152
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 0.9210944175720215,
      "learning_rate": 0.00017175843694493785,
      "loss": 0.5368,
      "step": 160
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 7.3726677894592285,
      "learning_rate": 0.0001703374777975133,
      "loss": 0.2783,
      "step": 168
    },
    {
      "epoch": 0.3128888888888889,
      "grad_norm": 25.22506332397461,
      "learning_rate": 0.00016891651865008883,
      "loss": 0.6678,
      "step": 176
    },
    {
      "epoch": 0.32711111111111113,
      "grad_norm": 23.09558868408203,
      "learning_rate": 0.00016749555950266431,
      "loss": 0.5045,
      "step": 184
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 18.932836532592773,
      "learning_rate": 0.00016607460035523977,
      "loss": 0.4716,
      "step": 192
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 54.22215270996094,
      "learning_rate": 0.0001646536412078153,
      "loss": 0.4012,
      "step": 200
    },
    {
      "epoch": 0.36977777777777776,
      "grad_norm": 19.967252731323242,
      "learning_rate": 0.00016323268206039078,
      "loss": 0.3467,
      "step": 208
    },
    {
      "epoch": 0.384,
      "grad_norm": 17.947147369384766,
      "learning_rate": 0.00016181172291296626,
      "loss": 0.4389,
      "step": 216
    },
    {
      "epoch": 0.3982222222222222,
      "grad_norm": 5.866796970367432,
      "learning_rate": 0.00016039076376554175,
      "loss": 0.2986,
      "step": 224
    },
    {
      "epoch": 0.41244444444444445,
      "grad_norm": 44.043983459472656,
      "learning_rate": 0.00015896980461811724,
      "loss": 0.2426,
      "step": 232
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 13.146349906921387,
      "learning_rate": 0.00015754884547069273,
      "loss": 0.3123,
      "step": 240
    },
    {
      "epoch": 0.4408888888888889,
      "grad_norm": 6.843648433685303,
      "learning_rate": 0.00015612788632326821,
      "loss": 0.2509,
      "step": 248
    },
    {
      "epoch": 0.45511111111111113,
      "grad_norm": 10.66215705871582,
      "learning_rate": 0.0001547069271758437,
      "loss": 0.4377,
      "step": 256
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 2.7859866619110107,
      "learning_rate": 0.0001532859680284192,
      "loss": 0.4606,
      "step": 264
    },
    {
      "epoch": 0.48355555555555557,
      "grad_norm": 21.793275833129883,
      "learning_rate": 0.00015186500888099468,
      "loss": 0.2933,
      "step": 272
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 34.70711898803711,
      "learning_rate": 0.00015044404973357017,
      "loss": 0.5194,
      "step": 280
    },
    {
      "epoch": 0.512,
      "grad_norm": 1.8494821786880493,
      "learning_rate": 0.00014902309058614565,
      "loss": 0.3508,
      "step": 288
    },
    {
      "epoch": 0.5262222222222223,
      "grad_norm": 40.75185775756836,
      "learning_rate": 0.00014760213143872114,
      "loss": 0.3461,
      "step": 296
    },
    {
      "epoch": 0.5404444444444444,
      "grad_norm": 2.967088222503662,
      "learning_rate": 0.00014618117229129663,
      "loss": 0.6,
      "step": 304
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 25.655637741088867,
      "learning_rate": 0.00014476021314387212,
      "loss": 0.3567,
      "step": 312
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 2.892897844314575,
      "learning_rate": 0.0001433392539964476,
      "loss": 0.1547,
      "step": 320
    },
    {
      "epoch": 0.5831111111111111,
      "grad_norm": 11.334778785705566,
      "learning_rate": 0.0001419182948490231,
      "loss": 0.3438,
      "step": 328
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.15580010414123535,
      "learning_rate": 0.00014049733570159858,
      "loss": 0.2263,
      "step": 336
    },
    {
      "epoch": 0.6115555555555555,
      "grad_norm": 1.8002104759216309,
      "learning_rate": 0.00013907637655417407,
      "loss": 0.1708,
      "step": 344
    },
    {
      "epoch": 0.6257777777777778,
      "grad_norm": 11.203950881958008,
      "learning_rate": 0.00013765541740674955,
      "loss": 0.2371,
      "step": 352
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.4549617767333984,
      "learning_rate": 0.00013623445825932507,
      "loss": 0.548,
      "step": 360
    },
    {
      "epoch": 0.6542222222222223,
      "grad_norm": 0.41362810134887695,
      "learning_rate": 0.00013481349911190053,
      "loss": 0.1989,
      "step": 368
    },
    {
      "epoch": 0.6684444444444444,
      "grad_norm": 8.878938674926758,
      "learning_rate": 0.00013339253996447602,
      "loss": 0.4865,
      "step": 376
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 50.52114486694336,
      "learning_rate": 0.00013197158081705153,
      "loss": 0.8141,
      "step": 384
    },
    {
      "epoch": 0.6968888888888889,
      "grad_norm": 19.517988204956055,
      "learning_rate": 0.000130550621669627,
      "loss": 0.2273,
      "step": 392
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 3.0872654914855957,
      "learning_rate": 0.00012912966252220248,
      "loss": 0.1928,
      "step": 400
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 23.655908584594727,
      "learning_rate": 0.000127708703374778,
      "loss": 0.2256,
      "step": 408
    },
    {
      "epoch": 0.7395555555555555,
      "grad_norm": 18.9208984375,
      "learning_rate": 0.00012628774422735348,
      "loss": 0.1011,
      "step": 416
    },
    {
      "epoch": 0.7537777777777778,
      "grad_norm": 16.356399536132812,
      "learning_rate": 0.00012486678507992894,
      "loss": 0.3515,
      "step": 424
    },
    {
      "epoch": 0.768,
      "grad_norm": 28.297103881835938,
      "learning_rate": 0.00012344582593250443,
      "loss": 0.249,
      "step": 432
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 21.77341079711914,
      "learning_rate": 0.00012202486678507993,
      "loss": 0.2522,
      "step": 440
    },
    {
      "epoch": 0.7964444444444444,
      "grad_norm": 46.82615280151367,
      "learning_rate": 0.00012060390763765543,
      "loss": 0.4201,
      "step": 448
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.5603654384613037,
      "learning_rate": 0.00011918294849023091,
      "loss": 0.4064,
      "step": 456
    },
    {
      "epoch": 0.8248888888888889,
      "grad_norm": 4.974957466125488,
      "learning_rate": 0.0001177619893428064,
      "loss": 0.2024,
      "step": 464
    },
    {
      "epoch": 0.8391111111111111,
      "grad_norm": 0.08399397134780884,
      "learning_rate": 0.0001163410301953819,
      "loss": 0.1707,
      "step": 472
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 5.2648797035217285,
      "learning_rate": 0.00011492007104795737,
      "loss": 0.1913,
      "step": 480
    },
    {
      "epoch": 0.8675555555555555,
      "grad_norm": 2.1909918785095215,
      "learning_rate": 0.00011349911190053286,
      "loss": 0.3628,
      "step": 488
    },
    {
      "epoch": 0.8817777777777778,
      "grad_norm": 0.07453923672437668,
      "learning_rate": 0.00011207815275310836,
      "loss": 0.1842,
      "step": 496
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.036974795162677765,
      "learning_rate": 0.00011065719360568385,
      "loss": 0.3407,
      "step": 504
    },
    {
      "epoch": 0.9102222222222223,
      "grad_norm": 20.006126403808594,
      "learning_rate": 0.00010923623445825932,
      "loss": 0.1515,
      "step": 512
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 42.37043762207031,
      "learning_rate": 0.00010781527531083482,
      "loss": 0.2582,
      "step": 520
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 42.61796188354492,
      "learning_rate": 0.00010639431616341031,
      "loss": 0.4427,
      "step": 528
    },
    {
      "epoch": 0.9528888888888889,
      "grad_norm": 0.47935646772384644,
      "learning_rate": 0.00010497335701598578,
      "loss": 0.1817,
      "step": 536
    },
    {
      "epoch": 0.9671111111111111,
      "grad_norm": 0.8691242933273315,
      "learning_rate": 0.00010355239786856129,
      "loss": 0.166,
      "step": 544
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 15.671932220458984,
      "learning_rate": 0.00010213143872113677,
      "loss": 0.471,
      "step": 552
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 41.62509536743164,
      "learning_rate": 0.00010071047957371227,
      "loss": 0.3304,
      "step": 560
    },
    {
      "epoch": 1.008888888888889,
      "grad_norm": 21.9136962890625,
      "learning_rate": 9.928952042628775e-05,
      "loss": 0.2406,
      "step": 568
    },
    {
      "epoch": 1.023111111111111,
      "grad_norm": 0.15528751909732819,
      "learning_rate": 9.786856127886324e-05,
      "loss": 0.3188,
      "step": 576
    },
    {
      "epoch": 1.0373333333333334,
      "grad_norm": 17.306089401245117,
      "learning_rate": 9.644760213143872e-05,
      "loss": 0.2223,
      "step": 584
    },
    {
      "epoch": 1.0515555555555556,
      "grad_norm": 13.00536060333252,
      "learning_rate": 9.502664298401421e-05,
      "loss": 0.1346,
      "step": 592
    },
    {
      "epoch": 1.0657777777777777,
      "grad_norm": 18.09334945678711,
      "learning_rate": 9.36056838365897e-05,
      "loss": 0.3464,
      "step": 600
    },
    {
      "epoch": 1.08,
      "grad_norm": 23.959014892578125,
      "learning_rate": 9.218472468916519e-05,
      "loss": 0.1343,
      "step": 608
    },
    {
      "epoch": 1.0942222222222222,
      "grad_norm": 0.8120296001434326,
      "learning_rate": 9.076376554174067e-05,
      "loss": 0.0879,
      "step": 616
    },
    {
      "epoch": 1.1084444444444443,
      "grad_norm": 21.93619728088379,
      "learning_rate": 8.934280639431618e-05,
      "loss": 0.2012,
      "step": 624
    },
    {
      "epoch": 1.1226666666666667,
      "grad_norm": 0.2651370167732239,
      "learning_rate": 8.792184724689165e-05,
      "loss": 0.263,
      "step": 632
    },
    {
      "epoch": 1.1368888888888888,
      "grad_norm": 21.823862075805664,
      "learning_rate": 8.650088809946715e-05,
      "loss": 0.2996,
      "step": 640
    },
    {
      "epoch": 1.1511111111111112,
      "grad_norm": 11.766258239746094,
      "learning_rate": 8.507992895204263e-05,
      "loss": 0.1448,
      "step": 648
    },
    {
      "epoch": 1.1653333333333333,
      "grad_norm": 1.4424540996551514,
      "learning_rate": 8.365896980461813e-05,
      "loss": 0.4157,
      "step": 656
    },
    {
      "epoch": 1.1795555555555555,
      "grad_norm": 0.8034982681274414,
      "learning_rate": 8.223801065719361e-05,
      "loss": 0.1693,
      "step": 664
    },
    {
      "epoch": 1.1937777777777778,
      "grad_norm": 16.14549446105957,
      "learning_rate": 8.081705150976909e-05,
      "loss": 0.1892,
      "step": 672
    },
    {
      "epoch": 1.208,
      "grad_norm": 6.8274688720703125,
      "learning_rate": 7.939609236234459e-05,
      "loss": 0.0575,
      "step": 680
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 3.9579524993896484,
      "learning_rate": 7.797513321492008e-05,
      "loss": 0.1189,
      "step": 688
    },
    {
      "epoch": 1.2364444444444445,
      "grad_norm": 0.09838251024484634,
      "learning_rate": 7.655417406749556e-05,
      "loss": 0.1114,
      "step": 696
    },
    {
      "epoch": 1.2506666666666666,
      "grad_norm": 24.0628719329834,
      "learning_rate": 7.513321492007105e-05,
      "loss": 0.1372,
      "step": 704
    },
    {
      "epoch": 1.264888888888889,
      "grad_norm": 28.81733512878418,
      "learning_rate": 7.371225577264654e-05,
      "loss": 0.5419,
      "step": 712
    },
    {
      "epoch": 1.279111111111111,
      "grad_norm": 0.1804332584142685,
      "learning_rate": 7.229129662522203e-05,
      "loss": 0.0631,
      "step": 720
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 10.473249435424805,
      "learning_rate": 7.087033747779752e-05,
      "loss": 0.0933,
      "step": 728
    },
    {
      "epoch": 1.3075555555555556,
      "grad_norm": 23.66754150390625,
      "learning_rate": 6.9449378330373e-05,
      "loss": 0.3058,
      "step": 736
    },
    {
      "epoch": 1.3217777777777777,
      "grad_norm": 40.20811080932617,
      "learning_rate": 6.802841918294849e-05,
      "loss": 0.0757,
      "step": 744
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.05606911703944206,
      "learning_rate": 6.660746003552398e-05,
      "loss": 0.1688,
      "step": 752
    },
    {
      "epoch": 1.3502222222222222,
      "grad_norm": 17.98202896118164,
      "learning_rate": 6.518650088809947e-05,
      "loss": 0.0773,
      "step": 760
    },
    {
      "epoch": 1.3644444444444446,
      "grad_norm": 0.47939008474349976,
      "learning_rate": 6.376554174067497e-05,
      "loss": 0.279,
      "step": 768
    },
    {
      "epoch": 1.3786666666666667,
      "grad_norm": 0.07977942377328873,
      "learning_rate": 6.234458259325044e-05,
      "loss": 0.0424,
      "step": 776
    },
    {
      "epoch": 1.3928888888888888,
      "grad_norm": 18.092437744140625,
      "learning_rate": 6.0923623445825936e-05,
      "loss": 0.2349,
      "step": 784
    },
    {
      "epoch": 1.407111111111111,
      "grad_norm": 27.890506744384766,
      "learning_rate": 5.9502664298401424e-05,
      "loss": 0.174,
      "step": 792
    },
    {
      "epoch": 1.4213333333333333,
      "grad_norm": 0.4671533703804016,
      "learning_rate": 5.808170515097692e-05,
      "loss": 0.0523,
      "step": 800
    },
    {
      "epoch": 1.4355555555555555,
      "grad_norm": 18.24981689453125,
      "learning_rate": 5.66607460035524e-05,
      "loss": 0.0386,
      "step": 808
    },
    {
      "epoch": 1.4497777777777778,
      "grad_norm": 0.27773499488830566,
      "learning_rate": 5.5239786856127887e-05,
      "loss": 0.0601,
      "step": 816
    },
    {
      "epoch": 1.464,
      "grad_norm": 2.594470977783203,
      "learning_rate": 5.381882770870338e-05,
      "loss": 0.0576,
      "step": 824
    },
    {
      "epoch": 1.478222222222222,
      "grad_norm": 0.0006492507527582347,
      "learning_rate": 5.239786856127886e-05,
      "loss": 0.2369,
      "step": 832
    },
    {
      "epoch": 1.4924444444444445,
      "grad_norm": 0.1813628226518631,
      "learning_rate": 5.0976909413854356e-05,
      "loss": 0.2632,
      "step": 840
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 8.978500366210938,
      "learning_rate": 4.955595026642984e-05,
      "loss": 0.085,
      "step": 848
    },
    {
      "epoch": 1.520888888888889,
      "grad_norm": 38.92768096923828,
      "learning_rate": 4.813499111900533e-05,
      "loss": 0.4964,
      "step": 856
    },
    {
      "epoch": 1.535111111111111,
      "grad_norm": 22.035877227783203,
      "learning_rate": 4.671403197158082e-05,
      "loss": 0.2819,
      "step": 864
    },
    {
      "epoch": 1.5493333333333332,
      "grad_norm": 1.9533194303512573,
      "learning_rate": 4.529307282415631e-05,
      "loss": 0.1681,
      "step": 872
    },
    {
      "epoch": 1.5635555555555556,
      "grad_norm": 0.33989959955215454,
      "learning_rate": 4.3872113676731795e-05,
      "loss": 0.1063,
      "step": 880
    },
    {
      "epoch": 1.5777777777777777,
      "grad_norm": 4.57857608795166,
      "learning_rate": 4.245115452930728e-05,
      "loss": 0.1045,
      "step": 888
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.9468908309936523,
      "learning_rate": 4.103019538188278e-05,
      "loss": 0.1459,
      "step": 896
    },
    {
      "epoch": 1.6062222222222222,
      "grad_norm": 27.198041915893555,
      "learning_rate": 3.9609236234458264e-05,
      "loss": 0.1596,
      "step": 904
    },
    {
      "epoch": 1.6204444444444444,
      "grad_norm": 25.281124114990234,
      "learning_rate": 3.8188277087033745e-05,
      "loss": 0.2895,
      "step": 912
    },
    {
      "epoch": 1.6346666666666667,
      "grad_norm": 0.013762100599706173,
      "learning_rate": 3.676731793960923e-05,
      "loss": 0.3662,
      "step": 920
    },
    {
      "epoch": 1.6488888888888888,
      "grad_norm": 0.010903248563408852,
      "learning_rate": 3.534635879218473e-05,
      "loss": 0.1716,
      "step": 928
    },
    {
      "epoch": 1.6631111111111112,
      "grad_norm": 0.6003363728523254,
      "learning_rate": 3.3925399644760215e-05,
      "loss": 0.2015,
      "step": 936
    },
    {
      "epoch": 1.6773333333333333,
      "grad_norm": 29.901721954345703,
      "learning_rate": 3.25044404973357e-05,
      "loss": 0.1556,
      "step": 944
    },
    {
      "epoch": 1.6915555555555555,
      "grad_norm": 23.009536743164062,
      "learning_rate": 3.108348134991119e-05,
      "loss": 0.362,
      "step": 952
    },
    {
      "epoch": 1.7057777777777776,
      "grad_norm": 7.877824783325195,
      "learning_rate": 2.966252220248668e-05,
      "loss": 0.128,
      "step": 960
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.038707271218299866,
      "learning_rate": 2.824156305506217e-05,
      "loss": 0.0836,
      "step": 968
    },
    {
      "epoch": 1.7342222222222223,
      "grad_norm": 22.585994720458984,
      "learning_rate": 2.682060390763766e-05,
      "loss": 0.2147,
      "step": 976
    },
    {
      "epoch": 1.7484444444444445,
      "grad_norm": 0.044725388288497925,
      "learning_rate": 2.539964476021314e-05,
      "loss": 0.2306,
      "step": 984
    },
    {
      "epoch": 1.7626666666666666,
      "grad_norm": 3.3190152645111084,
      "learning_rate": 2.3978685612788635e-05,
      "loss": 0.1314,
      "step": 992
    },
    {
      "epoch": 1.7768888888888887,
      "grad_norm": 16.876590728759766,
      "learning_rate": 2.255772646536412e-05,
      "loss": 0.1458,
      "step": 1000
    },
    {
      "epoch": 1.791111111111111,
      "grad_norm": 0.30086085200309753,
      "learning_rate": 2.113676731793961e-05,
      "loss": 0.2218,
      "step": 1008
    },
    {
      "epoch": 1.8053333333333335,
      "grad_norm": 0.07402032613754272,
      "learning_rate": 1.97158081705151e-05,
      "loss": 0.1527,
      "step": 1016
    },
    {
      "epoch": 1.8195555555555556,
      "grad_norm": 0.16121768951416016,
      "learning_rate": 1.8294849023090586e-05,
      "loss": 0.0294,
      "step": 1024
    },
    {
      "epoch": 1.8337777777777777,
      "grad_norm": 7.779963970184326,
      "learning_rate": 1.6873889875666074e-05,
      "loss": 0.0243,
      "step": 1032
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 19.240299224853516,
      "learning_rate": 1.545293072824156e-05,
      "loss": 0.2047,
      "step": 1040
    },
    {
      "epoch": 1.8622222222222222,
      "grad_norm": 0.07227501273155212,
      "learning_rate": 1.4031971580817053e-05,
      "loss": 0.1016,
      "step": 1048
    },
    {
      "epoch": 1.8764444444444446,
      "grad_norm": 26.2806453704834,
      "learning_rate": 1.2611012433392542e-05,
      "loss": 0.118,
      "step": 1056
    },
    {
      "epoch": 1.8906666666666667,
      "grad_norm": 5.723615646362305,
      "learning_rate": 1.119005328596803e-05,
      "loss": 0.3093,
      "step": 1064
    },
    {
      "epoch": 1.9048888888888889,
      "grad_norm": 9.357047080993652,
      "learning_rate": 9.769094138543517e-06,
      "loss": 0.0161,
      "step": 1072
    },
    {
      "epoch": 1.919111111111111,
      "grad_norm": 20.922916412353516,
      "learning_rate": 8.348134991119005e-06,
      "loss": 0.1535,
      "step": 1080
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 29.933616638183594,
      "learning_rate": 6.927175843694494e-06,
      "loss": 0.2083,
      "step": 1088
    },
    {
      "epoch": 1.9475555555555557,
      "grad_norm": 1.9488343000411987,
      "learning_rate": 5.506216696269983e-06,
      "loss": 0.1236,
      "step": 1096
    },
    {
      "epoch": 1.9617777777777778,
      "grad_norm": 0.19643034040927887,
      "learning_rate": 4.085257548845471e-06,
      "loss": 0.1117,
      "step": 1104
    },
    {
      "epoch": 1.976,
      "grad_norm": 21.02431869506836,
      "learning_rate": 2.664298401420959e-06,
      "loss": 0.1342,
      "step": 1112
    },
    {
      "epoch": 1.9902222222222221,
      "grad_norm": 0.02087128907442093,
      "learning_rate": 1.2433392539964477e-06,
      "loss": 0.0609,
      "step": 1120
    }
  ],
  "logging_steps": 8,
  "max_steps": 1126,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9925163089920000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
