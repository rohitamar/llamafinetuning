{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "925e1c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db73061c5c84ea7a1fbaebb87724ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "base_model = \"ministral/Ministral-3b-instruct\"\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"domenicrosati/TruthfulQA\", split=\"train\") \\\n",
    "            .shuffle(seed=42) \\\n",
    "            .select(range(2000))\n",
    "\n",
    "def make_prompt(x):\n",
    "    prompt = \"<|im_start|>user\\n\"\n",
    "    prompt += x[\"instruction\"]\n",
    "\n",
    "    if x[\"input\"].strip():\n",
    "        prompt += \"\\n\" + x[\"input\"]\n",
    "    \n",
    "    prompt += \"\\n<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    return prompt\n",
    "\n",
    "def tokenize_func(x):\n",
    "    text_tokens = tokenizer(\n",
    "        x[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    prompt = make_prompt(x)\n",
    "    prompt_tokens = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"text_input_ids\": text_tokens[\"input_ids\"],\n",
    "        \"text_attention_mask\": text_tokens[\"attention_mask\"],\n",
    "        \"prompt_input_ids\": prompt_tokens[\"input_ids\"],\n",
    "        \"prompt_attention_mask\": prompt_tokens[\"attention_mask\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9758ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f1634a9c8a4509b1e54a7adb012610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.025, seed=42)\n",
    "train = dataset['train']\n",
    "val = dataset['test']\n",
    "\n",
    "val = val.map(tokenize_func, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f24fc2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 11125.978988732668\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import math \n",
    "\n",
    "model.eval()\n",
    "total_loss = 0.0\n",
    "count = 0\n",
    "val_loader = DataLoader(val, batch_size=1)\n",
    "\n",
    "for batch in val_loader:\n",
    "    input_ids = torch.tensor(batch['text_input_ids'], device=device)\n",
    "    attention_mask = torch.tensor(batch['text_attention_mask'], device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "    \n",
    "    total_loss += outputs.loss.item()\n",
    "    count += 1\n",
    "\n",
    "ppl = math.exp(total_loss / count)\n",
    "print(f\"Perplexity: {ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f581e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: <|im_start|>user\n",
      "What does the following proverb mean: \"The grass is always greener on the other side\"?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "Model output: <|im_start|>user\n",
      "What does the following proverb mean: \"The grass is always greener on the other side\"?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The \"the grass is always green on the other side\" phrase is an epexplicit proverb about the nature forging new beginnings. The most likely explanation is that the phrase originates from Maynchickman Pittleton, which also is a postponing expression of youth in the early 18th century. It appears most presently in Charles Arthur Ghell's proverb of the first of the 17th century, \"It's never easy to be seen in the morning.\" In the early 17th century, it appears in Robert Keaton's proverb of the latter of the 17th century: \"You cannot see it anywhere there is water.\" Thus, \"the forest spells for you the quietest of morning.\"\n",
      "\n",
      "Here's a more direct interpretation: The phrase is a part of the proverb of Mary Chan and Elizabeth Keaton, especially \"it's never good to know you're never able to see it.\" This phrase, also expressed in Elizabeth Powell's proverb, \"if someone is honest and honest again you can say it's always brightly brightly.\" So in the early 20th century, the phrase\n",
      "\n",
      "Expected output: The phrase \"the grass is always greener on the other side\" is a proverb that means that what you don't have appears better than what you do have. It is often used to indicate that people tend to think that something they don't possess is better than what they already have.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for batch in val_loader:\n",
    "    input_ids = torch.tensor(batch['prompt_input_ids'], device=device)\n",
    "    attention_mask = torch.tensor(batch['prompt_attention_mask'], device=device)\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"PROMPT: {batch['prompt'][0]}\", end = \"\\n\\n\")\n",
    "    print(f\"Model output: {decoded_output}\", end = \"\\n\\n\")\n",
    "    print(f\"Expected output: {batch['output'][0]}\", end = \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03f7ad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training \n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    r=4, \n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.05, \n",
    "    bias='none', \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b32897cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3457664234884810971c366deb242ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1950 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_fn(x):\n",
    "    prompt = make_prompt(x)\n",
    "    entire_input = f\"{prompt}\\n{x['output']}\"\n",
    "    tokens = tokenizer(\n",
    "        entire_input,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=True\n",
    "    )\n",
    "    tokens['labels'] = tokens['input_ids'].copy()\n",
    "    return tokens\n",
    "\n",
    "train = train.map(preprocess_fn, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05639ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='244' max='244' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [244/244 14:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.897400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.946600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.788800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.799200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.802100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.786100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.666700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.776700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.707500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.687700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.691300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.740800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.736800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.654600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.672200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.619900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.774400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.582300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.656800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.763900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.767000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.760300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.679700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.673100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.811200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.690300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.764500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.712900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.554900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.616500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.580900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.719700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.728600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.668300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.722700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.745500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.729400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>1.730500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.557800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>1.512200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.642200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.480700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.658600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>1.736000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.686700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "def checkpoint_forward(fn, *args):\n",
    "    return checkpoint(fn, *args, use_reentrant=False)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.gradient_checkpointing = True\n",
    "\n",
    "torch.utils.checkpoint.checkpoint = checkpoint_forward\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"adamw_torch\", \n",
    "    logging_steps=5, \n",
    "    learning_rate=2e-4, \n",
    "    fp16=True,\n",
    "    warmup_ratio=0.1, \n",
    "    lr_scheduler_type=\"linear\", \n",
    "    num_train_epochs=1, \n",
    "    gradient_checkpointing=True, \n",
    "    save_strategy=\"epoch\", \n",
    "    label_names=[\"labels\"]\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model, \n",
    "    train_dataset=train, \n",
    "    args=training_args\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002bc1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
